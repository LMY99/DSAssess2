# Answer

I performed 100 repeated test with different learning rates from $10^{-4}$ to $10^0=1$. The gradient descent algorithm is designed to stop when the absolute change in one update is less than $10^{-6}$ or more than $10^5$ iterations have been done. The latter is implemented to prevent indefinitely long runtime, and typically means that the algorithm is converging too slow for any practical use. The value of $x\in\mathbb{R}^{10}$ is sampled as independent standard normal variables, and $y=x+\epsilon$ where $\epsilon$ is made up of 10 $N(0,0.1^2)$ variables. The result from the gradient descent is compared to the true value $\frac{x\cdot y}{x\cdot x}$.

As shown in the attached graphics PDF file (GD_Diagnosis.pdf), as the learning rate increases, the number of iterations until convergence, given that the algorithm does converge, first decreases then stays flat. The difference between the algorithm result and analytic solution also decreases as learning rate increases. However, when the learning rate is larger than $10^{-0.5}$, the algorithm will begin to fail with a large probability, and will completely fail as learning rate further increases. This is due to the algorithm overshooting the true minimum point, as shown in the last plot, where the trace of the algorithm repeatedly cross the red line depicting the true value, and the update is becoming more and more radical. The reason for this is that when the learning rate is large and the gradient value is also large, the update will have a large step size.